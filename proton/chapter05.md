
# å¼·åŒ–å­¦ç¿’ã‚¼ãƒŸ 5ç«  å®Ÿè£…ãƒ‘ãƒ¼ãƒˆ

## è‡ªå·±ç´¹ä»‹
---
* å·å³¶ ä¸¸ç”Ÿ
* proton( @proton_1602) <font color="Gainsboro">ã¡ãªã¿ã«ã€1602ã¯é›»æ°—ç´ é‡ã®ä¸Š4æ¡ã§ã™ã‚ˆã€‚</font>
* ç ”ç©¶å®¤ã¯ã€AY, trok, nemr, mnmt, kwhrå½“ãŸã‚Šï¼Ÿã¾ã æœªå®šã€‚ä»®ç¬¬ä¸€ã¯AYã€‚
* æ©Ÿæ¢°å­¦ç¿’ã¯å»å¹´ã®æ˜¥ã®ã‚¼ãƒŸä»¥é™ã¡ã‚‡ã“ã¡ã‚‡ã“ã‚„ã£ã¦ãŸã‚Šã™ã‚‹ã€‚ãƒã‚¤ãƒˆã‚‚ä¸€å¿œæ©Ÿæ¢°å­¦ç¿’é–¢ä¿‚
* å¼·åŒ–å­¦ç¿’ã¯ä½•ã‚‚ã‚„ã£ã¦ã“ãªã‹ã£ãŸã®ã§åˆã‚ã¦ã§ã™ã€‚é–“é•ã£ã¦ã‚‹ã€æ€ªã—ã„æ‰€ãŒã‚ã‚‹æ™‚ã¯ã¡ã‚ƒã‚“ã¨æŒ‡æ‘˜ã—ã¦ã»ã—ã„ã€‚

## 5ç« ã®ãƒ¡ãƒ¢
<font color="Gainsboro">è‹±å¼±ãŒã‚ã–ã‚ã–è‹±èªã®æœ¬ã‚’ä½•åº¦ã‚‚å¿…è¦ãªã„æ‰€ã¾ã§èª­ã¿è¿”ã•ãªãã¦æ¸ˆã‚€ã‚ˆã†ã«æ›¸ã„ãŸãƒ¡ãƒ¢</font>

***
5ç« ã¯p122~141(p99~118)  
* FrozenLakeã§ã®ä¾‹(p122~124),å¸¸ã«æœ€å–„ã®å ±é…¬ã‚’å¾—ã‚‹è²ªæ¬²æ³•ã ã¨ã†ã¾ãè¡Œã‹ãªã„ã¨ãã‚‚ã‚ã‚‹ã‚ˆã£ã¦ã„ã†ã€ãŸã ã®å°å…¥
* ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼(p125~127), å°†æ¥ã®å³æ™‚å ±é…¬å’Œã‚’å†å¸°çš„ãªå¼ã‚’åˆ©ç”¨ã—ã¦éå»ã®å³æ™‚å ±é…¬ã‚’ä½¿ã†ã“ã¨ã§ä»£ç”¨andé«˜é€ŸåŒ–?, ç¢ºç‡ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ç¢ºç‡çš„ãªå ±é…¬ç­‰ã«å¯¾å¿œã§ãã‚‹ã‚ˆã€ã™ã”ã„ã­  
    * p125: æ±ºå®šçš„æ–¹ç­–, p126~127:ç¢ºç‡çš„æ–¹ç­–ã®å°å…¥ã€ä¸€èˆ¬çš„ãªãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼, p127:(çŠ¶æ…‹)è¡Œå‹•ä¾¡å€¤Qã®å°å…¥
    * p125: $ V_0(a=a_i)$ã¯ãªã‚“ã‹çŠ¶æ…‹ä¾¡å€¤ã®ãã›ã«æ–¹ç­–å¼•æ•°?ã«ã—ã¦ã¦å¤‰ãªæ„Ÿã˜ã ãŒã€æ±ºå®šçš„æ–¹ç­–$\pi$ã‚’å–ã£ãŸæ™‚ã®$V_\pi(s_0)$ã¨åŒã˜?
    * p126: $S$ã¯$s_0$ã‹ã‚‰é·ç§»å¯èƒ½ãªçŠ¶æ…‹é›†åˆã€$r_{s,a}$ã¯è¡Œå‹•aã«ã‚ˆã‚ŠçŠ¶æ…‹sã«é·ç§»ã—ãŸæ™‚ã®å³æ™‚å ±é…¬ã€$V_s$ã¯çŠ¶æ…‹sã®çŠ¶æ…‹ä¾¡å€¤ã€‚$A$ã¯çŠ¶æ…‹$s_0$ã‹ã‚‰å–ã‚Šã†ã‚‹è¡Œå‹•é›†åˆ  
* ç°¡å˜ãªä¾‹ã«ã‚ˆã‚‹Qå­¦ç¿’ã®èª¬æ˜,ä¾¡å€¤åå¾©æ³•(p128~129), è¡Œå‹•ã«å¯¾ã™ã‚‹çŠ¶æ…‹é·ç§»ã‚’ç¢ºç‡çš„ã«ã—ã¦ã‚‹ã€‚
    * p129: Qå€¤,è¡Œå‹•ã®è©•ä¾¡ã‚’å­¦ç¿’ã—ã¦ã€è¡Œå‹•ã®è©•ä¾¡ãŒæœ€å¤§ã«ãªã‚‹è¡Œå‹•ã‚’é¸æŠã™ã‚‹æ–¹ç­–ã‚’å–ã‚‹Valueãƒ™ãƒ¼ã‚¹ã€‚é·ç§»ç¢ºç‡ã¨ã‹ãŒæ—¢çŸ¥ãªå®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã¯çã—ã„ã®ã§ã€äºˆæ¸¬ã—ã¦ã„ãå¿…è¦ã‚‚ã‚ã‚‹ã‚ˆã€‚
* ãƒ«ãƒ¼ãƒ—ç­‰ã‚’å«ã‚€ä¸€èˆ¬çš„ãªãƒãƒ«ã‚³ãƒ•éç¨‹ã«ãŠã‘ã‚‹V,Qã®æ±‚ã‚æ–¹(p129~131), ã“ã®æ–¹æ³•ã¯çŠ¶æ…‹ç©ºé–“ãŒé›¢æ•£çš„ã§ååˆ†å°ã•ã„å¿…è¦ãŒã‚ã‚‹ãŒã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸQå­¦ç¿’ã‚’ç”¨ã„ã‚‹ã¨ãªã‚“ã¨ã‹ãªã‚‹(å¾Œç« )ã€é·ç§»ç¢ºç‡ã‚’æ™®é€šã¯çŸ¥ã‚‰ãªã„ã‹ã‚‰ã‚ã‚‹çŠ¶æ…‹æ¨ç§»ã¨è¡Œå‹•æ™‚ã®å ±é…¬ã‚’è¦šãˆã¦ãŠãå¿…è¦ã‚‚ã‚ã‚‹ã€‚
    1. $V_i, Q_{s,a}$ã®åˆæœŸåŒ–(0)
    2. å…¨ã¦ã®çŠ¶æ…‹$s$ã«ã¤ã„ã¦ã®çŠ¶æ…‹ä¾¡å€¤$V_s$ã€å…¨ã¦ã®çŠ¶æ…‹$s$,è¡Œå‹•$a$ã«ã¤ã„ã¦ã®è¡Œå‹•ä¾¡å€¤$Q_{s,a}$ã‚’ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ã§æ›´æ–°  
    $V_s \leftarrow \max_\alpha \sum_{s'}p_{a,s\rightarrow s'}(r_{s,a}+\gamma V_{s'})$  
    $Q_{s,a} \leftarrow \sum_{s'}p_{a,s\rightarrow s'}(r_{s,a}+\gamma \max Q_{s',a'})$
    3. ååˆ†ãªå›æ•°ã‹å·®åˆ†ãŒä¸€å®šå€¤ä»¥ä¸‹ã«ãªã‚‹ã¾ã§2ã‚’ãƒ«ãƒ¼ãƒ— 
* FrozenLakeã§ã®ä¾‹(p132~139)(ã‚³ãƒ¼ãƒ‰æœ‰)
    * 01_frozenlake_v_iteration.py p132~137: ä¾¡å€¤åå¾©æ³•ã€ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ³•ã«æ¯”ã¹ã¦ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒå®Œå…¨ã«çµ‚äº†ã™ã‚‹å¿…è¦ãŒãªãã€å„æ–½è¡Œã§æ¨å®š,æ›´æ–°ãŒã§ãã‚‹ã®ã§åŠ¹ç‡çš„ã§åæŸãŒæ—©ã„ã€‚FrozenLakeã¯çµ‚äº†æ™‚ã®ã¿å ±é…¬ãŒå…¥ã‚‹ã®ã§ãƒãƒƒãƒ—ãŒåºƒã„ã¨ã¡ã‚‡ã£ã¨æ™‚é–“ã‹ã‹ã‚‹ã€‚"FrozenLake8x8-v0"ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã§æ¯”è¼ƒã—ãŸã®ãŒå›³9
    * 02_frozenlake_q_iteration.py p138~139: Qå­¦ç¿’ã€01ã§ã¯çŠ¶æ…‹ä¾¡å€¤Vã‚’è¡¨ã«æŒã£ã¦ã„ãŸãŒã€è¡Œå‹•ä¾¡å€¤Qã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã«æŒã¤ã€‚ã“ã‚Œã«ã‚ˆã‚Šãƒ¡ãƒ¢ãƒªã®ä½¿ç”¨é‡ã¯å¢—ãˆã‚‹ãŒæ›´æ–°ãŒåˆ†ã‹ã‚Šã‚„ã™ããªã‚‹ã€‚Vã®æ›´æ–°ã‚ˆã‚Šã‚‚ã€Qã®æ›´æ–°ã®ã»ã†ãŒä¸€èˆ¬çš„ã§ã¯ã‚ã‚‹ãŒã€Actor-Criticæ³•ç­‰ã§ã¯Vã®æ›´æ–°ã‚’ç”¨ã„ã‚‹ã€‚
* ã¾ã¨ã‚(p140), RLã§é‡è¦ãªV,Q,ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ã¨å€¤åå¾©ã«ã‚ˆã‚‹æ”¹å–„ã‚’å­¦ã‚“ã ã€‚æ¬¡ã¯ãƒ‡ã‚£ãƒ¼ãƒ—Qãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

## ã‚³ãƒ¼ãƒ‰ã¨è§£èª¬(?)

## FrozenLakeã®ãƒ«ãƒ¼ãƒ«
---
* 4x4ã®ç›¤é¢ã‚’ç§»å‹•ã™ã‚‹ï¼(FrozenLake8x8-v0ã§ã¯8x8)
* SãŒé–‹å§‹åœ°ç‚¹ã§ï¼ŒGãŒã‚´ãƒ¼ãƒ«ï¼
* HãŒè½ã¨ã—ç©´ã§ã‚²ãƒ¼ãƒ å¤±æ•—ã§ï¼ŒFã¯åºŠã§ç§»å‹•ã§ãã‚‹ï¼
* éš£æ¥4æ–¹å‘ã«ç§»å‹•å¯èƒ½
* ç¾åœ¨ã®ä½ç½®ã¨ã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼ã‹ã©ã†ã‹ãŒåˆ†ã‹ã‚‹ï¼
* æ„å›³ã—ãŸæ–¹å‘ã«é€²ã‚ã‚‹ç¢ºç‡ã¯1/3ã§ï¼Œãã‚Œä»¥å¤–ã ã¨90åº¦ç›´è§’ã«é€²ã‚€ï¼
* env.reset()ã—ã¦ã‚‚ãƒãƒƒãƒ—ã¯å¤‰ã‚ã‚‰ãªã„ï¼

### 01_frozenlake_v_iteration.py
---


```python
#01_frozenlake_v_iteration.py
import gym
import collections
from tensorboardX import SummaryWriter

ENV_NAME = "FrozenLake-v0"
#ENV_NAME = "FrozenLake8x8-v0"
GAMMA = 0.9
TEST_EPISODES = 20


#Agentã‚¯ãƒ©ã‚¹ã®å®šç¾©ã€èª­ã‚ã°ã ã„ãŸã„ã‚ã‹ã‚‹
class Agent:
    def __init__(self):
        self.env = gym.make(ENV_NAME)
        self.state = self.env.reset()
        self.rewards = collections.defaultdict(float)
        self.transits = collections.defaultdict(collections.Counter)
        self.values = collections.defaultdict(float)

    def play_n_random_steps(self, count):
        #countå›ã ã‘é·ç§»ã™ã‚‹ã€é€”ä¸­ã§çµ‚äº†ã™ã‚Œã°åˆæœŸåŒ–ã—ã¦å†é–‹
        #ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ³•ã¨ç•°ãªã‚Šã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçµ‚äº†ã—ãªãã¨ã‚‚ã‚ˆã„
        for _ in range(count):
            #action_space.sample()ã§å¯èƒ½ãªè¡Œå‹•ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¿”ã—ã¦ãã‚Œã‚‹
            action = self.env.action_space.sample()
            
            #stepãƒ¡ã‚½ãƒƒãƒ‰ã«è¡Œå‹•ã‚’æ¸¡ã™ã¨ã€çŠ¶æ…‹,å ±é…¬,çµ‚äº†åˆ¤å®š,ãƒ‡ãƒãƒƒã‚°ç”¨æƒ…å ±ã‚’è¿”ã™ã€‚
            #ex: observation,reward,done,info=env.step(action)
            new_state, reward, is_done, _ = self.env.step(action)
            
            #å ±é…¬è¡¨(Reward table)ã®æ›´æ–°ã€keyã¯çŠ¶æ…‹,è¡Œå‹•,æ¬¡çŠ¶æ…‹ã§valueã¯å³æ™‚å ±é…¬
            self.rewards[(self.state, action, new_state)] = reward
            
            #é·ç§»è¡¨(Transitions table)ã®æ›´æ–°
            #ã‚ã‚‹çŠ¶æ…‹sã¨è¡Œå‹•aã«ã‚ˆã‚ŠçŠ¶æ…‹s'ã«è¡Œã£ãŸå›æ•°ã‚’è¨˜éŒ²ã™ã‚‹ã“ã¨ã§ã€
            #æœªçŸ¥ã®é·ç§»ç¢ºç‡ã‚’æ¨å®šã™ã‚‹ã€‚
            self.transits[(self.state, action)][new_state] += 1
            
            #çµ‚äº†ã—ã¦ã„ãŸå ´åˆã¯resetã—ã¦åˆæœŸçŠ¶æ…‹ã®observationã‚’è¿”ã™
            #ãã†ã˜ã‚ƒãªã„ãªã‚‰stepã«ã‚ˆã‚Šå¾—ã‚‰ã‚ŒãŸæ–°çŠ¶æ…‹ã‚’å…¥ã‚Œã‚‹ã€‚
            self.state = self.env.reset() if is_done else new_state

    #é·ç§»è¡¨ã§é·ç§»ç¢ºç‡ã‚’è¿‘ä¼¼ã—ã¦è¡Œå‹•ä¾¡å€¤Qã‚’è¨ˆç®—ã™ã‚‹ã€‚
    #è¨ˆç®—ã«ã¯çŠ¶æ…‹ä¾¡å€¤Vã‚’ç”¨ã„ã‚‹ã€‚
    def calc_action_value(self, state, action):
        target_counts = self.transits[(state, action)]
        total = sum(target_counts.values())
        action_value = 0.0
        for tgt_state, count in target_counts.items():
            reward = self.rewards[(state, action, tgt_state)]
            action_value += (count / total) * (reward + GAMMA * self.values[tgt_state])
        return action_value

    #è¡Œå‹•ä¾¡å€¤QãŒæœ€ã‚‚é«˜ããªã‚‹è¡Œå‹•aã‚’è¿”ã™
    def select_action(self, state):
        best_action, best_value = None, None
        for action in range(self.env.action_space.n):
            action_value = self.calc_action_value(state, action)
            if best_value is None or best_value < action_value:
                best_value = action_value
                best_action = action
        return best_action

    #æ¨å®šã•ã‚ŒãŸè¡Œå‹•ä¾¡å€¤Qã«åŸºã¥ã„ãŸæ–¹ç­–Ï€ã§
    #1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œã—ç´¯è¨ˆå ±é…¬ã‚’è¿”ã™
    #å­¦ç¿’å¾Œã®æç”»ã‚’ã—ãŸã„ã®ã§å°‘ã—å¤‰æ›´ã—ã¦ã¾ã™
    def play_episode(self, env,moniter=0):
        total_reward = 0.0
        state = env.reset()
        cnt = 0
        while True:
            cnt += 1
            action = self.select_action(state)
            new_state, reward, is_done, _ = env.step(action)
            self.rewards[(state, action, new_state)] = reward
            self.transits[(state, action)][new_state] += 1
            total_reward += reward
            if moniter:
                env.render()
            if is_done:
                break
            state = new_state
        if moniter:
            return total_reward, cnt
        else: 
            return total_reward

    #æ–¹ç­–ã¯QãŒæœ€å¤§ã«ãªã‚‹è¡Œå‹•ã¨ã—ã¦ã€çŠ¶æ…‹ä¾¡å€¤Vã®è¡¨ã‚’æ›´æ–°ã™ã‚‹ã€‚
    #è¡Œå‹•ä¾¡å€¤Qã‚’ç”¨ã„ã¦ã„ã‚‹ãŒå±•é–‹ã™ã‚Œã°Vã®ãƒ™ãƒ«ãƒãƒ³æœ€é©æ–¹ç¨‹å¼ã«ãªã£ã¦ã‚‹
    def value_iteration(self):
        for state in range(self.env.observation_space.n):
            state_values = [self.calc_action_value(state, action)
                            for action in range(self.env.action_space.n)]
            self.values[state] = max(state_values)


if __name__ == "__main__":
    test_env = gym.make(ENV_NAME)
    agent = Agent()
    writer = SummaryWriter(comment="-v-iteration")

    iter_no = 0
    best_reward = 0.0
    while True:
        iter_no += 1
        #100å›ãƒ©ãƒ³ãƒ€ãƒ ã«å‹•ã„ã¦æ¢ç´¢ã€è¡¨åŸ‹ã‚
        agent.play_n_random_steps(100)
        agent.value_iteration()

        #TEST_EPISODESå›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦å¹³å‡ã‚’å–ã‚‹
        #å¹³å‡ãŒä¸€å®šå€¤ã‚’è¶…ãˆã¦ã„ãŸã‚‰å­¦ç¿’çµ‚äº†
        reward = 0.0
        for _ in range(TEST_EPISODES):
            reward += agent.play_episode(test_env)
        reward /= TEST_EPISODES
        writer.add_scalar("reward", reward, iter_no)
        if reward > best_reward:
            print("Best reward updated %.3f -> %.3f" % (best_reward, reward))
            best_reward = reward
        if reward > 0.80:
            print("Solved in %d iterations!" % iter_no)
            break
    writer.close()

```

    Best reward updated 0.000 -> 0.150
    Best reward updated 0.150 -> 0.200
    Best reward updated 0.200 -> 0.300
    Best reward updated 0.300 -> 0.450
    Best reward updated 0.450 -> 0.600
    Best reward updated 0.600 -> 0.750
    Best reward updated 0.750 -> 0.800
    Best reward updated 0.800 -> 0.900
    Solved in 14 iterations!


#### å­¦ç¿’å‰ã¨å­¦ç¿’å¾Œã®æ¯”è¼ƒ
---
* å®Ÿè¡Œã™ã‚‹ã¨ãƒ©ãƒ³ãƒ€ãƒ ãªæ–¹ç­–ã¨å­¦ç¿’å¾Œã®æ–¹ç­–ã§ã®ã€1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å‹•ããŒè¦‹ã‚Œã¾ã™ã€‚
(ç’°å¢ƒæ§‹ç¯‰æ¸ˆã¿ã®pcã§ã‚„ã‚‰ãªã„ã¨ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦å±¥æ­´ãŒæ¶ˆãˆã¡ã‚ƒã†ã‚ˆ)
* ç·‘æœ¬ã¿ãŸãVãƒ†ãƒ¼ãƒ–ãƒ«ã®å›³ç¤ºã¨ã‹ã‚„ã‚ŠãŸã‹ã£ãŸã‘ã©ä½™ã‚Šã‚„ã‚‹æ„å‘³ã‚’æ„Ÿã˜ãªã‹ã£ãŸã€‚æš‡ãŒæœ‰ã£ãŸã‚‰ã‚„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„


```python
test_env.reset()
cnt = 0
total_reward = 0.0
while True:
    cnt += 1
    action=test_env.action_space.sample()
    observation,reward,done,info = test_env.step(action)
    total_reward += reward
    test_env.render()
    if done:
        print("---randam_policy(1 episode)---")
        print("play: {}, total_reward:{}".format(cnt,total_reward))
        break
```

      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Down)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Up)
    SFFF
    F[41mH[0mFH
    FFFH
    HFFG
    ---randam_policy(1 episode)---
    play: 4, total_reward:0.0



```python
state = test_env.reset()
total_reward, cnt = agent.play_episode(test_env,moniter=1)
print("---opt_policy(1 episode)---")
print("play: {}, total_reward:{}".format(iter_no,total_reward))
```

      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    FHFH
    F[41mF[0mFH
    HFFG
      (Down)
    SFFF
    FHFH
    FF[41mF[0mH
    HFFG
      (Left)
    SFFF
    FHFH
    FFFH
    HF[41mF[0mG
      (Down)
    SFFF
    FHFH
    FFFH
    HFF[41mG[0m
    ---opt_policy(1 episode)---
    play: 14, total_reward:1.0


### 02_frozenlake_q_iteration.py
---


```python
#02_frozenlake_q_iteration.py
import gym
import collections
from tensorboardX import SummaryWriter

ENV_NAME = "FrozenLake-v0"
#ENV_NAME = "FrozenLake8x8-v0"
GAMMA = 0.9
TEST_EPISODES = 20


#diffå–ã‚Œã°ã‚ã‹ã‚‹ã‘ã©01ã¨ã»ã¨ã‚“ã©é•ã„ãªã„
class Agent:
    def __init__(self):
        self.env = gym.make(ENV_NAME)
        self.state = self.env.reset()
        self.rewards = collections.defaultdict(float)
        self.transits = collections.defaultdict(collections.Counter)
        self.values = collections.defaultdict(float)

    def play_n_random_steps(self, count):
        for _ in range(count):
            action = self.env.action_space.sample()
            new_state, reward, is_done, _ = self.env.step(action)
            self.rewards[(self.state, action, new_state)] = reward
            self.transits[(self.state, action)][new_state] += 1
            self.state = self.env.reset() if is_done else new_state

    #calc_action_value æ¶ˆå»
            
    def select_action(self, state):
        best_action, best_value = None, None
        for action in range(self.env.action_space.n):
            #ç›´æ¥Qãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿æŒã—ã¦ã„ã‚‹ã®ã§calc_action_valueã¯å¿…è¦ãªã„ã€€
            action_value = self.values[(state, action)]
            if best_value is None or best_value < action_value:
                best_value = action_value
                best_action = action
        return best_action

        #å­¦ç¿’å¾Œã®æç”»ã‚’ã—ãŸã„ã®ã§å°‘ã—å¤‰æ›´ã—ã¦ã¾ã™
    def play_episode(self, env,moniter=0):
        total_reward = 0.0
        state = env.reset()
        cnt = 0
        while True:
            cnt += 1
            action = self.select_action(state)
            new_state, reward, is_done, _ = env.step(action)
            self.rewards[(state, action, new_state)] = reward
            self.transits[(state, action)][new_state] += 1
            total_reward += reward
            if moniter:
                env.render()
            if is_done:
                break
            state = new_state
        if moniter:
            return total_reward, cnt
        else: 
            return total_reward
    
    #Qãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°
    def value_iteration(self):
        for state in range(self.env.observation_space.n):
            for action in range(self.env.action_space.n):
                action_value = 0.0
                target_counts = self.transits[(state, action)]
                total = sum(target_counts.values())
                for tgt_state, count in target_counts.items():
                    reward = self.rewards[(state, action, tgt_state)]
                    best_action = self.select_action(tgt_state)
                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])
                self.values[(state, action)] = action_value


if __name__ == "__main__":
    test_env = gym.make(ENV_NAME)
    agent = Agent()
    writer = SummaryWriter(comment="-q-iteration")

    iter_no = 0
    best_reward = 0.0
    while True:
        iter_no += 1
        agent.play_n_random_steps(100)
        agent.value_iteration()

        reward = 0.0
        for _ in range(TEST_EPISODES):
            reward += agent.play_episode(test_env)
        reward /= TEST_EPISODES
        writer.add_scalar("reward", reward, iter_no)
        if reward > best_reward:
            print("Best reward updated %.3f -> %.3f" % (best_reward, reward))
            best_reward = reward
        if reward > 0.80:
            print("Solved in %d iterations!" % iter_no)
            break
    writer.close()

```

    Best reward updated 0.000 -> 0.250
    Best reward updated 0.250 -> 0.600
    Best reward updated 0.600 -> 0.650
    Best reward updated 0.650 -> 0.700
    Best reward updated 0.700 -> 0.850
    Solved in 15 iterations!


#### å­¦ç¿’å‰ã¨å­¦ç¿’å¾Œã®æ¯”è¼ƒ
---
* å®Ÿè¡Œã™ã‚‹ã¨ãƒ©ãƒ³ãƒ€ãƒ ãªæ–¹ç­–ã¨å­¦ç¿’å¾Œã®æ–¹ç­–ã§ã®ã€1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å‹•ããŒè¦‹ã‚Œã¾ã™ã€‚
(ç’°å¢ƒæ§‹ç¯‰æ¸ˆã¿ã®pcã§ã‚„ã‚‰ãªã„ã¨ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦å±¥æ­´ãŒæ¶ˆãˆã¡ã‚ƒã†ã‚ˆ)


```python
test_env.reset()
cnt = 0
total_reward = 0.0
while True:
    cnt += 1
    action=test_env.action_space.sample()
    observation,reward,done,info = test_env.step(action)
    total_reward += reward
    test_env.render()
    if done:
        print("---randam_policy(1 episode)---")
        print("play: {}, total_reward:{}".format(cnt,total_reward))
        break
```

      (Right)
    S[41mF[0mFF
    FHFH
    FFFH
    HFFG
      (Up)
    S[41mF[0mFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Right)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Right)
    SFFF
    F[41mH[0mFH
    FFFH
    HFFG
    ---randam_policy(1 episode)---
    play: 6, total_reward:0.0



```python
state = test_env.reset()
total_reward, cnt = agent.play_episode(test_env,moniter=1)
print("---opt_policy(1 episode)---")
print("play: {}, total_reward:{}".format(iter_no,total_reward))
```

      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    [41mS[0mFFF
    FHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    FHFH
    F[41mF[0mFH
    HFFG
      (Down)
    SFFF
    FHFH
    FF[41mF[0mH
    HFFG
      (Left)
    SFFF
    FHFH
    F[41mF[0mFH
    HFFG
      (Down)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    [41mF[0mHFH
    FFFH
    HFFG
      (Left)
    SFFF
    FHFH
    [41mF[0mFFH
    HFFG
      (Up)
    SFFF
    FHFH
    F[41mF[0mFH
    HFFG
      (Down)
    SFFF
    FHFH
    FF[41mF[0mH
    HFFG
      (Left)
    SFFF
    FHFH
    FFFH
    HF[41mF[0mG
      (Down)
    SFFF
    FHFH
    FFFH
    H[41mF[0mFG
      (Right)
    SFFF
    FHFH
    FFFH
    H[41mF[0mFG
      (Right)
    SFFF
    FHFH
    FFFH
    H[41mF[0mFG
      (Right)
    SFFF
    FHFH
    FFFH
    HF[41mF[0mG
      (Down)
    SFFF
    FHFH
    FFFH
    HF[41mF[0mG
      (Down)
    SFFF
    FHFH
    FFFH
    H[41mF[0mFG
      (Right)
    SFFF
    FHFH
    FFFH
    HF[41mF[0mG
      (Down)
    SFFF
    FHFH
    FFFH
    HF[41mF[0mG
      (Down)
    SFFF
    FHFH
    FFFH
    H[41mF[0mFG
      (Right)
    SFFF
    FHFH
    FFFH
    H[41mF[0mFG
      (Right)
    SFFF
    FHFH
    FFFH
    H[41mF[0mFG
      (Right)
    SFFF
    FHFH
    F[41mF[0mFH
    HFFG
      (Down)
    SFFF
    FHFH
    FFFH
    H[41mF[0mFG
      (Right)
    SFFF
    FHFH
    FFFH
    HF[41mF[0mG
      (Down)
    SFFF
    FHFH
    FFFH
    HF[41mF[0mG
      (Down)
    SFFF
    FHFH
    FFFH
    HFF[41mG[0m
    ---opt_policy(1 episode)---
    play: 15, total_reward:1.0


ãªã‚“ã‹è¿½åŠ ã—ã¦å®Ÿè£…ã—ã‚ˆã†ã‹ã¨æ€ã£ãŸã‘ã©ã€ã“ã“ã¾ã§ã®ç¯„å›²ã ã¨FrozenLakeã¿ãŸã„ãªå•é¡Œã—ã‹è§£ã‘ãªã•ãã†ãªã®ã§ã‚„ã‚ã¾ã—ãŸã€‚


```python

```
